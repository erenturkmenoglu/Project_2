# -*- coding: utf-8 -*-
"""Project 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d-XQfFhYG3vDy5YrcT9T_Nba0PVkzmFL
"""

#Title
print("Project 2: Machine Learning")

#Colab Installations
!pip install praw

# Importing libraries
import praw
import pandas as pd
from praw.models import MoreComments
import datetime

### BEGINNING OF PSEUDOCODE
# Upload a daily scrape of the WSB subreddit (for day 1 - NOV 26) using Reddit API

#Name "today" variable
today = datetime.date.today()
print(today)

# Authenticating Reddit account
reddit = praw.Reddit(client_id='ynA1ypO4kMVwRA', client_secret='dtFwv4-mgMDeI9uPm0GPieCRaFn8Dw', user_agent='Reddit_NLP')

subreddit_entry = input('Please enter a subreddit :')

# Pulling hot posts
hot_posts = reddit.subreddit(subreddit_entry).hot(limit=600)
for post in hot_posts:
    print(post.title, post.url)

# Extracting all features of subreddit posts
posts = []
subreddit = reddit.subreddit(subreddit_entry)
for post in subreddit.hot(limit=600):
    posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext])
posts = pd.DataFrame(posts,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body'])
#posts = posts.sort_values('score', ascending=False).set_index('id')

posts.to_csv(f'/content/wallstreetbets_hots_df_{today}.csv')

##### Clean the data - this will be done with "nested for loop" - step 2 Separate into comments and add to a dataframe
reddit_comments = pd.read_csv(f'/content/wallstreetbets_hots_df_{today}.csv')
#
reddit_comments.head(20)

#Create a PANDAS dataframe of the ticker symbols we're looking for (or just all the ticker symbols)
NASDAQ_TICKERS = pd.read_csv('/content/NASQAQ tickers CSV.csv', engine='python', header=None).reset_index()
NASDAQ_TICKERS.columns = ["Nasdaq_Ticker", "Company_Name"]
NASDAQ_TICKERS.head(20
          )

#Get dataframe with both NASDAQ tickers and TEXT column from oracle text. Unrelated columns so use Ignore Index. Dont forget to drop before training model.
bjorn_test_df = pd.concat([reddit_comments, NASDAQ_TICKERS], axis=0, ignore_index=True)
bjorn_test_df

#define function to iterate through column of NASDAQ tickers, and look for instances of the items in column "text"
def check_subset(text,Nasdaq_Ticker):
  s = []
  for q in Nasdaq_Ticker:
    for r in text:
      if ((str(q) in str(r))) & (len(str(q))>3):
        s.append([q,r])


  return s

#check dytpes to verify success
bjorn_test_df.dtypes

#Initiate function
ticker_name_find_in_comments = check_subset(bjorn_test_df['title'], bjorn_test_df['Nasdaq_Ticker'])
ticker_name_find_in_comments

with open('extract_to_ibm.txt', 'w',encoding="utf8") as f:
    for item in ticker_name_find_in_comments:
        f.write("%s\n" % item)

#change to pd dataframe
December_11_Extracted_df = pd.DataFrame(ticker_name_find_in_comments)
#Save as csv
December_11_Extracted_df.to_csv('December_10_Extracted_CSV', index=False)

#Read saved CSV
December_11_Extracted_csv = pd.read_csv('December_10_Extracted_CSV', index_col=False)
December_11_Extracted_csv.columns = ["Ticker_Present_In_Text", "Hot_Post"]
December_11_Extracted_csv

!pip install nltk

#Step 3 run VADER sentiment
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer, PorterStemmer
import nltk
import pandas as pd
import numpy

nltk.download('vader_lexicon')
# Initialize the VADER sentiment analyzer
analyzer = SentimentIntensityAnalyzer()

# Create a sentiment scores DataFrame
topic_sentiments = []

for comment in December_11_Extracted_csv['Hot_Post']:
    try:
        text_ = comment
        sentiment = analyzer.polarity_scores(text_)
        compound = sentiment["compound"]
        pos = sentiment["pos"]
        neu = sentiment["neu"]
        neg = sentiment["neg"]
        
        topic_sentiments.append({
            "hot_post": text_,
            "compound": compound,
            "positive": pos,
            "negative": neg,
            "neutral": neu
            
        })
        
    except AttributeError:
        pass
    
sentiments_df = pd.DataFrame(topic_sentiments)

# Printing the output
sentiments_df

#Concat with Ticker_Present_In_Text from above December_3_Extracted_csv
VADER_scores_df = pd.concat([December_11_Extracted_csv,sentiments_df], axis=1)
VADER_scores_df.drop(columns='hot_post')

VADER_scores_df = VADER_scores_df.groupby('Ticker_Present_In_Text').mean()

"""# IBM Watson"""

! pip install ibm-watson
! pip install ibm-cloud-sdk-core

! pip install python-dotenv

from ibm_watson import NaturalLanguageUnderstandingV1
#from ibm_watson.natural_language_understanding_v1 import Features, EntitiesOptions, KeywordsOptions, EmotionsOptions, SentimentOptions
#from ibm_watson.natural_language_understanding_v1 import Features, EntitiesOptions, KeywordsOptions, SentimentOptions
from ibm_watson.natural_language_understanding_v1 import Features, CategoriesOptions
from ibm_watson.natural_language_understanding_v1 import Features, SentimentOptions
from ibm_watson.natural_language_understanding_v1 import Features, KeywordsOptions
from ibm_watson.natural_language_understanding_v1 import Features, EntitiesOptions
import os
import json
from pandas import json_normalize
from ibm_watson import ToneAnalyzerV3
from ibm_cloud_sdk_core.authenticators import IAMAuthenticator
from dotenv import load_dotenv
import json
from pandas import json_normalize
load_dotenv()

sent_api = "******"
sent_url = "******"

authenticator = IAMAuthenticator(sent_api)

natural_language_understanding = NaturalLanguageUnderstandingV1(
    version='2020-08-01',
    authenticator=authenticator
)

natural_language_understanding.set_service_url(sent_url)

#####OUTSIDE OF PYTHON -- Convert /content/December_10_Extracted_CSV csv to text file and re-upload####
#Then proceed

fh = open('/content/extract_to_ibm.txt',encoding="utf8")
sentence_tone = pd.DataFrame()
#check=[]
for line in fh:
    response = natural_language_understanding.analyze(
    text=line,
    content_type="application/json",
    content_language="en",
    accept_language="en",
    language='en',
    features=Features(
        entities=EntitiesOptions(emotion=True, sentiment=True, limit=2),
        keywords=KeywordsOptions(emotion=True, sentiment=True,
                                 limit=10))).get_result()
    sentence_tone=sentence_tone.append(json_normalize(data=response["keywords"]))
fh.close()

#sentence_tone.head(20)
sentence_tone = sentence_tone.groupby('text').mean()
sentence_tone

VADER_scores_df

#Vader_Fixed = VADER_scores_df.set_index('Ticker_Present_In_Text')
#IBM_Fixed = sentence_tone.set_index('text')

#concat final df for model
FINAL_DF_FOR_MODEL = pd.concat([VADER_scores_df, sentence_tone], axis=1, join='inner')
FINAL_DF_FOR_MODEL.to_csv('final_sentiments_df')
FINAL_DF_FOR_MODEL

"""# Word Cloud"""

!pip3 install wordcloud
!pip3 install matplotlib
from wordcloud import WordCloud
import matplotlib.pyplot as plt

#Create a graphic wordcloud
text_for_wordcloud = str(ticker_name_find_in_comments)
cloud = WordCloud().generate(text_for_wordcloud)
plt.figure(figsize=(8, 8), facecolor=None)
plt.imshow(cloud, interpolation="bilinear")
plt.axis("off")
plt.tight_layout(pad=0)
plt.show()

#Highlight ticker symbols that were associated with positive VADER sentiment scores
VADER_scores_df.sort_values(by=['compound'], inplace=True, ascending=False)
VADER_scores_df.head()

#VADER_scores_df_12_05

#Highlight ticker symbols that were associated with positive VADER sentiment scores
VADER_scores_df.sort_values(by=['compound'], inplace=True, ascending=True)
VADER_scores_df.head()

#consolidate positives and negatives into two dataframes
December_10_long_df = VADER_scores_df.loc[VADER_scores_df['compound'] > 0 , :].copy()
December_10_long_df.head()

December_10_long_df.to_csv('December_10_long_Portfolio_CSV', index=False)

December_10_short_df = VADER_scores_df.loc[VADER_scores_df['compound'] < 0 , :].copy()
December_10_short_df.head()

December_10_short_df.to_csv('December_10_Short_Portfolio_CSV', index=False)

#construct a portfolio of the long stocks on the given day (day 1 - Dec 3) and find the closing values of those stocks on Day 1 - google sheets?
December_3_long_df['NASDAQ_TICKERS']

#construct a portfolio of the short stocks on the given day (day 1 - Dec 3)  and find the closing values of those stocks on Day 1 - google sheets?
December_3_short_df['NASDAQ_TICKERS']

"""# Closing Price"""

!pip install quandl

#Read csv
sentiments_final = pd.read_csv('final_sentiments_df').rename(columns={'Unnamed: 0': 'Tickers'})
import quandl
quandl.ApiConfig.api_key = '*****'

results = []
for i, r in sentiments_final.iterrows():
    result = quandl.get('EOD/'+r["Tickers"], column_index = '11', start_date='2020-12-9', end_date='2020-12-10')
    result['% Change'] = result.pct_change()
    results.append(result)

prices_12_9 = pd.concat(results).reset_index()
prices_12_9_drop = prices_12_9.dropna().reset_index()
prices_12_9_drop.head()

Closing_price_df_final = pd.concat([sentiments_final, prices_12_9_drop], axis=1).drop(columns=['Date'])
Closing_price_df_final = Closing_price_df_final.drop(columns=['index'])
Closing_price_df_final

"""### Model 1 - Deep Learning to predict closing price via WallStreetBets sentiment

"""

# Commented out IPython magic to ensure Python compatibility.
 # Initial imports
from pathlib import Path
import pandas as pd
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# %matplotlib inline

Closing_price_df_for_model = Closing_price_df_final.drop(columns=['Tickers'])
Closing_price_df_for_model.head()

# Create the features (X) and target (y) sets
X = Closing_price_df_for_model.iloc[:, 0:13].values
y = Closing_price_df_for_model["% Change"].values

X.shape

# Scale the data
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler().fit(X)
X = scaler.transform(X)

# Define the model - shallow neural net
number_hidden_nodes = 9
number_input_features = 13

nn = Sequential()
# Hidden layer
nn.add(
    Dense(units=number_hidden_nodes, input_dim=number_input_features, activation="relu")
)
# Output layer
nn.add(Dense(units=1, activation="linear"))

# Compile the model
nn.compile(loss="mean_squared_error", optimizer="adam", metrics=["mse"])

# Train the model
model_1 = nn.fit(X, y, validation_split=0.2, epochs=200)

# Plot the train and test loss function
plt.plot(model_1.history["loss"])
plt.title("loss_function - 1 hidden layer")
plt.legend(["loss"])
plt.show()

# Define the model - deep neural net
number_input_features = 13
hidden_nodes_layer1 = 18
hidden_nodes_layer2 = 6

nn = Sequential()
# First hidden layer
nn.add(
    Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation="relu")
)
# Second hidden layer
nn.add(Dense(units=hidden_nodes_layer2, activation="relu"))
# Output layer
nn.add(Dense(units=1, activation="linear"))

# Compile model
nn.compile(loss="mean_squared_error", optimizer="adam", metrics=["mse"])

# Fit the model
model_2 = nn.fit(X, y, validation_split=0.3, epochs=200)

plt.plot(model_1.history["loss"])
plt.plot(model_2.history["loss"])
plt.title("loss_function - Training")
plt.legend(["1 hidden layer", "2 hidden layers"])
plt.show()

# Train vs test for shallow net
plt.plot(model_1.history["loss"])
plt.plot(model_1.history["val_loss"])
plt.title("loss_function - Training Vs. Validation - 1 hidden layer")
plt.legend(["train", "test"])
plt.show()

# Train vs test for deep net
plt.plot(model_2.history["loss"])
plt.plot(model_2.history["val_loss"])
plt.title("loss_function - Training Vs. Validation - 2 hidden layers")
plt.legend(["train", "test"])
plt.show()

# Save model as JSON
nn_json = nn.to_json()

file_path = Path("model.json")
with open(file_path, "w") as json_file:
    json_file.write(nn_json)

# Save weights
file_path = "model.h5"
nn.save_weights("model.h5")

# Load the saved model to make predictions
from tensorflow.keras.models import model_from_json

# load json and create model
file_path = Path("model.json")
with open(file_path, "r") as json_file:
    model_json = json_file.read()
loaded_model = model_from_json(model_json)

# load weights into new model
file_path = "model.h5"
loaded_model.load_weights(file_path)

# Make some predictions with the loaded model
Closing_price_df_for_model["predicted"] = loaded_model.predict(X)
Closing_price_df_for_model.head(51)

!pip install mpl_finance
import matplotlib.pyplot as plt
import pandas as pd

Closing_price_df_for_graph = Closing_price_df_for_model.drop(columns=['compound',	'positive',	'negative',	'neutral',	'relevance',	'count',	'sentiment.score',	'emotion.sadness',	'emotion.joy',	'emotion.fear',	'emotion.disgust',	'emotion.anger',	'Adj_Close'])

Closing_price_df_for_graph.plot(kind = 'bar', figsize = [25,8],  ylim = [-0.1,0.6])

# Predict values using the testing data
from sklearn.metrics import mean_squared_error

y_pred = loaded_model.predict(X)
 
 # Evaluate the model with the MSE metric
print(f"The Mean Squared Error of our model is: {mean_squared_error(y, y_pred)}")

"""# IBM Watson Sentiment - Stock Price Prediction"""

sadness=Closing_price_df_final['emotion.sadness'].max()
joy=Closing_price_df_final['emotion.joy'].max()
fear=Closing_price_df_final['emotion.fear'].max()
disgust=Closing_price_df_final['emotion.disgust'].max()
anger=Closing_price_df_final['emotion.anger'].max()

sadness_stocks_df=Closing_price_df_final.loc[Closing_price_df_final['emotion.sadness']== sadness]
joy_stocks_df=Closing_price_df_final.loc[Closing_price_df_final['emotion.joy']== joy]
fear_stocks_df=Closing_price_df_final.loc[Closing_price_df_final['emotion.fear']== fear]
disgust_stocks_df=Closing_price_df_final.loc[Closing_price_df_final['emotion.disgust']== disgust]
anger_stocks_df=Closing_price_df_final.loc[Closing_price_df_final['emotion.anger']== anger]

import alpaca_trade_api as tradeapi
import os
import pandas as pd

alpaca_api_key = "PK7HMY0NMZGCCGKY1DGZR"
alpaca_secret_key = "HnKDzBGE0oDUzRIKhmaBr59KMv0haa69EPnGgOI8"

api = tradeapi.REST(alpaca_api_key,alpaca_secret_key,api_version = "v2")
timeframe = "1D"
start_date = pd.Timestamp("2017-12-07", tz="America/New_York").isoformat()
end_date = pd.Timestamp("2020-12-09", tz="America/New_York").isoformat()

def stock_details(tickers):
    df_ticker = api.get_barset(
    tickers,
    timeframe,
    start=start_date,
    end=end_date).df
    #df_stocks = pd.DataFrame(df_tickers)    
    return df_ticker

"""# Sadness Stock"""

sadness_stocks_df.head()

stock_code=sadness_stocks_df.iloc[0]
tickers=stock_code[0]
tickers

sadness_ticker = pd.DataFrame(stock_details(tickers))

sadness_ticker.head()

sadness_ticker.index=sadness_ticker.index.date
sadness_ticker=sadness_ticker.droplevel(axis=1,level=0)
sadness_ticker.head()

"""# Joy Stock"""

joy_stocks_df.head()

stock_code=joy_stocks_df.iloc[0]
tickers=stock_code[0]
tickers

joy_ticker = pd.DataFrame(stock_details(tickers))

joy_ticker.head()

joy_ticker.index=joy_ticker.index.date
joy_ticker=joy_ticker.droplevel(axis=1,level=0)
joy_ticker.head()

"""# Fear Stock"""

fear_stocks_df.head()

stock_code=fear_stocks_df.iloc[0]
tickers=stock_code[0]
tickers

fear_ticker = pd.DataFrame(stock_details(tickers))

fear_ticker.head()

fear_ticker.index=fear_ticker.index.date
fear_ticker=fear_ticker.droplevel(axis=1,level=0)
fear_ticker.head()

"""# Disgust Stock"""

disgust_stocks_df.head()

stock_code=disgust_stocks_df.iloc[0]
tickers=stock_code[0]
tickers

disgust_ticker.head()

disgust_ticker.index=disgust_ticker.index.date
disgust_ticker=disgust_ticker.droplevel(axis=1,level=0)
disgust_ticker.head()

"""# Anger Stock"""

anger_stocks_df.head()

stock_code=anger_stocks_df.iloc[0]
tickers=stock_code[0]
tickers

anger_ticker = pd.DataFrame(stock_details(tickers))

anger_ticker.head()

anger_ticker.index=anger_ticker.index.date
anger_ticker=anger_ticker.droplevel(axis=1,level=0)
anger_ticker.head()

"""## LSTM Stock Predictor Using Closing Prices - Disgust Stock"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
from pathlib import Path

# %matplotlib inline

from numpy.random import seed

seed(1)
from tensorflow import random

random.set_seed(2)

def window_data(disgust_ticker, window, feature_col_number, target_col_number):
    """
    This function accepts the column number for the features (X) and the target (y).
    It chunks the data up with a rolling window of Xt - window to predict Xt.
    It returns two numpy arrays of X and y.
    """
    X = []
    y = []
    for i in range(len(disgust_ticker) - window):
        features = disgust_ticker.iloc[i : (i + window), feature_col_number]
        target = disgust_ticker.iloc[(i + window), target_col_number]
        X.append(features)
        y.append(target)
    return np.array(X), np.array(y).reshape(-1, 1)

# Creating the features (X) and target (y) data using the window_data() function.
window_size = 5

feature_column = 3
target_column = 3
X, y = window_data(disgust_ticker, window_size, feature_column, target_column)
print (f"X sample values:\n{X[:5]} \n")
print (f"y sample values:\n{y[:5]}")

# Use 70% of the data for training and the remainder for testing
split = int(0.7 * len(X))
X_train = X[: split]
X_test = X[split:]
y_train = y[: split]
y_test = y[split:]

# Use the MinMaxScaler to scale data between 0 and 1.
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(X)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)
scaler.fit(y)
y_train = scaler.transform(y_train)
y_test = scaler.transform(y_test)

# Reshape the features for the model
X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))
print (f"X_train sample values:\n{X_train[:5]} \n")
print (f"X_test sample values:\n{X_test[:5]}")

# Import required Keras modules
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

# Define the LSTM RNN model.
model = Sequential()

number_units = 5
dropout_fraction = 0.2

# Layer 1
model.add(LSTM(
    units=number_units,
    return_sequences=True,
    input_shape=(X_train.shape[1], 1))
    )
model.add(Dropout(dropout_fraction))
# Layer 2
model.add(LSTM(units=number_units, return_sequences=True))
model.add(Dropout(dropout_fraction))
# Layer 3
model.add(LSTM(units=number_units))
model.add(Dropout(dropout_fraction))
# Output layer
model.add(Dense(1))

# Compile the model
model.compile(optimizer="adam", loss="mean_squared_error")

# Summarize the model
model.summary()

# Train the model
model.fit(X_train, y_train, epochs=20, shuffle=False, batch_size=1, verbose=1)

# Evaluate the model
model.evaluate(X_test, y_test)

# Make some predictions
predicted = model.predict(X_test)

# Recover the original prices instead of the scaled version
predicted_prices = scaler.inverse_transform(predicted)
real_prices = scaler.inverse_transform(y_test.reshape(-1, 1))

# Create a DataFrame of Real and Predicted values
stocks = pd.DataFrame({
    "Real": real_prices.ravel(),
    "Predicted": predicted_prices.ravel()
    }, index = disgust_ticker.index[-len(real_prices): ])
stocks.head()

# Plot the real vs predicted prices as a line chart
stocks.plot()